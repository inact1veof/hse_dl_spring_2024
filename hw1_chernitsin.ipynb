{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Домашняя работа №1**"
      ],
      "metadata": {
        "id": "q4dWlWI0WWui"
      },
      "id": "q4dWlWI0WWui"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выполнил Черницин И. А. МПМИИ231"
      ],
      "metadata": {
        "id": "FdZ-tlDPWbTi"
      },
      "id": "FdZ-tlDPWbTi"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Задание 1**"
      ],
      "metadata": {
        "id": "TX3AcxVKWdhx"
      },
      "id": "TX3AcxVKWdhx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Необходимо написать градиентный спуск для линейной регрессии\n",
        "\n",
        "Для реализации будем использовать MSE, проверим реализацию и сравним с sklearn реализацией"
      ],
      "metadata": {
        "id": "-6XQuwnOWieg"
      },
      "id": "-6XQuwnOWieg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d8fe1a9",
      "metadata": {
        "id": "3d8fe1a9"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e34a92c4",
      "metadata": {
        "id": "e34a92c4"
      },
      "outputs": [],
      "source": [
        "class GDLR:\n",
        "    '''\n",
        "    Linear regression model with gradient descent\n",
        "\n",
        "    :param learning_rate: loss func speed\n",
        "    :param max_iter: number of iteration of alrorithm\n",
        "    :param loss_history: error values for iterations\n",
        "    :param w: model weights\n",
        "    '''\n",
        "    def __init__(self, learning_rate=0.01, max_iter=500):\n",
        "        self.lr = learning_rate\n",
        "        self.max_iter = max_iter\n",
        "        self.loss_history = {'iteration' : [], 'loss' : []}\n",
        "        self.w = None\n",
        "    def _calc_loss(self, X, y):\n",
        "        '''\n",
        "        Method for loss calculation\n",
        "\n",
        "        :param X: feature dataset\n",
        "        :param y: target dataset\n",
        "\n",
        "        :return loss value\n",
        "        '''\n",
        "        y_pred = np.dot(X, self.w)\n",
        "        loss = np.sum((y_pred - y) ** 2) / len(y)\n",
        "        return loss\n",
        "    def _calc_gradient(self, X, y):\n",
        "        '''\n",
        "        Method for gradient calculation\n",
        "\n",
        "        :param X: feature dataset\n",
        "        :param y: target dataset\n",
        "\n",
        "        :return new gradient vec\n",
        "        '''\n",
        "        gradient = (2 / X.shape[0]) * np.dot(X.T, (np.dot(X, self.w) - y))\n",
        "        return gradient\n",
        "    def _update_weights(self, gradient):\n",
        "        '''\n",
        "        Method makes new weights with gradient and lr values\n",
        "\n",
        "        :param gradient: new gradient vec\n",
        "        '''\n",
        "        self.w -= self.lr * gradient\n",
        "    def fit(self, X, y):\n",
        "        '''\n",
        "        Method for fit LR model using GD\n",
        "\n",
        "        :param X: feature dataset\n",
        "        :param y: target dataset\n",
        "\n",
        "        :return None\n",
        "        '''\n",
        "        n_features = X.shape[1]\n",
        "        self.w = np.zeros(n_features)\n",
        "\n",
        "        for iteration in range(self.max_iter):\n",
        "            gradient = self._calc_gradient(X, y)\n",
        "\n",
        "            self._update_weights(gradient)\n",
        "\n",
        "            loss = self._calc_loss(X, y)\n",
        "            self.loss_history['iteration'].append(iteration)\n",
        "            self.loss_history['loss'].append(loss)\n",
        "    def predict(self, X):\n",
        "        '''\n",
        "        Method for make predict using model weights\n",
        "\n",
        "        :param X: feature dataset\n",
        "\n",
        "        :return result values array\n",
        "        '''\n",
        "        return np.dot(X, self.w)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b74b5531",
      "metadata": {
        "id": "b74b5531"
      },
      "outputs": [],
      "source": [
        "# Проверим на простой генерации от sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f91f0d5",
      "metadata": {
        "id": "6f91f0d5"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7efe8191",
      "metadata": {
        "id": "7efe8191"
      },
      "outputs": [],
      "source": [
        "X, y = make_regression(n_samples=1000, n_features=5, noise=1, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2ffaed0",
      "metadata": {
        "id": "a2ffaed0",
        "outputId": "902e6abf-a4d6-4d41-90a2-bcd58bd9a7cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MSE =  1.1090609683464694\n",
            "CPU times: user 13.5 ms, sys: 1.58 ms, total: 15 ms\n",
            "Wall time: 12.1 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "model = LinearRegression()\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"MSE = \", mean_squared_error(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50ed81c7",
      "metadata": {
        "id": "50ed81c7",
        "outputId": "9e83eeec-2f81-4daf-a1d0-a80eefd7f0ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MSE =  1.1111520865886781\n",
            "CPU times: user 27.1 ms, sys: 0 ns, total: 27.1 ms\n",
            "Wall time: 25.9 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "my_model = GDLR()\n",
        "\n",
        "my_model.fit(X_train, y_train)\n",
        "y_pred = my_model.predict(X_test)\n",
        "\n",
        "print(\"MSE = \", mean_squared_error(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c08c192a",
      "metadata": {
        "id": "c08c192a"
      },
      "outputs": [],
      "source": [
        "# В результате получили качество, сопоставимое с sklearn, времени затрачено больше, но задачи оптимизации не стояло"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 2"
      ],
      "metadata": {
        "id": "zLDSDNbvWoIG"
      },
      "id": "zLDSDNbvWoIG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Необходимо написать метод обратного распространения ошибки для MLP (многослойный персептрон)\n",
        "\n",
        "Для простоты используем линейную активацию (производная = 1)"
      ],
      "metadata": {
        "id": "3SfgjWg6Wra8"
      },
      "id": "3SfgjWg6Wra8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b872e626",
      "metadata": {
        "id": "b872e626"
      },
      "outputs": [],
      "source": [
        "class MLPRegressor:\n",
        "    '''\n",
        "    Regressor class using MLP and backpropagation\n",
        "\n",
        "    :param input_size: size of input layer (number of features)\n",
        "    :param hiddes_sizes: sizes of hidden layers ([layers_count, neurons_count])\n",
        "    :param output_size: size of output vec\n",
        "    :param learning_rate: loss func speed\n",
        "    :param max_iter: number of iteration of alrorithm\n",
        "    :param loss_history: error values for iterations\n",
        "    :param weights: model weights\n",
        "    :param biases: model biases\n",
        "    '''\n",
        "    def __init__(self, input_size, hidden_sizes, output_size, learning_rate=0.01, max_iter=100):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_sizes = hidden_sizes\n",
        "        self.output_size = output_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_iter = max_iter\n",
        "        self.loss_history = {'iteration' : [], 'loss' : []}\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "        layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
        "        for i in range(len(layer_sizes)-1):\n",
        "            self.weights.append(np.random.randn(layer_sizes[i], layer_sizes[i+1]))\n",
        "            self.biases.append(np.zeros(layer_sizes[i+1]))\n",
        "\n",
        "    def _linear_activation(self, x):\n",
        "        '''\n",
        "        Activation func for mlp\n",
        "\n",
        "        :param x: input value vec before activation\n",
        "\n",
        "        :return value vec after activation\n",
        "        '''\n",
        "        return x\n",
        "\n",
        "    def _compute_loss(self, y_true, y_pred):\n",
        "        '''\n",
        "        Method for calculating MSE\n",
        "\n",
        "        :param y_true: real target values\n",
        "        :param y_pred: predicted target values\n",
        "\n",
        "        :return mse vec\n",
        "        '''\n",
        "        return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "    def _forward_pass(self, X):\n",
        "        '''\n",
        "        Method for make forward pass\n",
        "\n",
        "        :param X: feature dataset\n",
        "\n",
        "        :return output values\n",
        "        '''\n",
        "        layer_output = X\n",
        "        self.layer_outputs = [layer_output]\n",
        "        for i in range(len(self.weights)):\n",
        "            layer_output = np.dot(layer_output, self.weights[i]) + self.biases[i]\n",
        "            if i < len(self.weights) - 1:\n",
        "                layer_output = self._linear_activation(layer_output)\n",
        "            self.layer_outputs.append(layer_output)\n",
        "        return layer_output\n",
        "\n",
        "    def _backward_pass(self, X, y_true, y_pred):\n",
        "        '''\n",
        "        Method for make backward pass\n",
        "\n",
        "        :param X: feature dataset\n",
        "        :param y_true: real target values\n",
        "        :param y_pred: predicted target values\n",
        "\n",
        "        :return gradients and biases\n",
        "        '''\n",
        "        gradients_weights = []\n",
        "        gradients_biases = []\n",
        "\n",
        "        error = 2 * (y_pred - y_true) / y_true.shape[0]\n",
        "        for i in range(len(self.weights) - 1, -1, -1):\n",
        "            if i == len(self.weights) - 1:\n",
        "                # Градиенты для выходного слоя\n",
        "                gradients_weights.insert(0, np.dot(self.layer_outputs[i].T, error))\n",
        "                gradients_biases.insert(0, np.sum(error, axis=0))\n",
        "            else:\n",
        "                # Градиенты для внутренних слоев\n",
        "                error = np.dot(error, self.weights[i+1].T)\n",
        "                gradients_weights.insert(0, np.dot(self.layer_outputs[i].T, error))\n",
        "                gradients_biases.insert(0, np.sum(error, axis=0))\n",
        "                error = error * 1  # Производная линейной активации = 1\n",
        "\n",
        "        return gradients_weights, gradients_biases\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        '''\n",
        "        Method for model fit using forward pass and backward pass\n",
        "\n",
        "        :param X: feature dataset\n",
        "        :param y: target dataset\n",
        "\n",
        "        :return None\n",
        "        '''\n",
        "        for iteration in range(self.max_iter):\n",
        "            y_pred = self._forward_pass(X)\n",
        "\n",
        "            gradients_weights, gradients_biases = self._backward_pass(X, y, y_pred)\n",
        "\n",
        "            for i in range(len(self.weights)):\n",
        "                self.weights[i] -= self.learning_rate * gradients_weights[i]\n",
        "                self.biases[i] -= self.learning_rate * gradients_biases[i]\n",
        "\n",
        "            loss = self._compute_loss(y, y_pred)\n",
        "            self.loss_history['iteration'].append(iteration)\n",
        "            self.loss_history['loss'].append(loss)\n",
        "\n",
        "    def predict(self, X):\n",
        "        '''\n",
        "        Method for make predict using forward pass\n",
        "\n",
        "        :param X: feature dataset\n",
        "\n",
        "        :return result values array\n",
        "        '''\n",
        "        return self._forward_pass(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d9bc8d3",
      "metadata": {
        "id": "7d9bc8d3",
        "outputId": "30ca7f80-c45b-4887-e655-5042a11cff78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MSE =  23.874807917208614\n"
          ]
        }
      ],
      "source": [
        "X, y = make_regression(n_samples=1000, n_features=5, noise=1, random_state=42)\n",
        "\n",
        "y = y.reshape(-1,1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=42)\n",
        "\n",
        "mlp_regressor = MLPRegressor(input_size=X_train.shape[1], hidden_sizes=[15], output_size=y_train.shape[1])\n",
        "mlp_regressor.fit(X_train, y_train)\n",
        "\n",
        "y_pred = mlp_regressor.predict(X_test)\n",
        "\n",
        "print(\"MSE = \", mean_squared_error(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44f995c4",
      "metadata": {
        "id": "44f995c4"
      },
      "outputs": [],
      "source": [
        "# Получаем примерно схожие значения"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}